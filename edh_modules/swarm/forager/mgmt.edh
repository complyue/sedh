
import * 'net'

import * 'swarm/RT'

# work definition scripts are allowed to change the inferred configuration at
# 'swarm/ENV', import the module as a reference, to get/set effective artifacts
# living there
senv = { import _ 'swarm/ENV' }

import * './wsq'


# worksource connection, and the team of workers employed by it
export class WSC {

  method __init__ (
    forager as this.forager,
    addr as this.addr,
    hcPlanned as this.hcPlanned,
    executable as this.executable,
    workDir as this.workDir,
    workModu as this.workModu,
    priority as this.priority,
  ) {
    this.forager =: Forager()

    # headcount reserved for this worksource
    this.hcReserved = 0
    # headcount hired by the worksource, updated from the worksource,
    # as its demands changed
    this.hcEmployed = sink

    # all live worker processes by pid
    this.workers = {}

    # stop signal for this wsc
    this.stopped = sink

    # sparking sink of signals for reform requests
    # note lingering copy of the event sink is discarded, for convenience of
    # all for-from-do loops against `this.reformSignal` to be subsequential,
    # but be aware `this.reformSignal.mrv` will always return nil
    this.reformSignal = sink.subseq
  }

  method __repr__() {
    '<WorkTeamFor ' ++ this.addr ++ ' ' ++ this.workers.size ++ '/'
    ++ this.hcEmployed.mrv ++ '/' ++ this.hcReserved
    ++ '/' ++ this.hcPlanned ++ '>'
  }

  method key() repr( this.addr )

  method engage ( hcAvailable ) {
    this.hcReserved = min( hcAvailable, this.hcPlanned )

    {

      go { # do the negotiation on a separate thread, don't block the forager from comm with other worksources concurrently

        # necessary cleanup after it's disconnected anyway
        defer this.stop()

        # make the connection
        let clnt = Client(
          addrSpec= this.addr,
          useSandbox= false, # trade security mitigation for simplicity
        ) $@ () => {
          import * that # expose all exported instance methods

          peer = perform @netPeer
          peer =: Peer()
          console.debug<| 'Swarm forager negotiating with worksource - ' ++ peer
          defer {
            console.debug<| 'Disconnecting worksource ' ++ peer
          }

          effect {
            ; @dataSink = peer.armChannel( dataChan, this.hcEmployed )
          }


          # we'll identify ourself as a forager to the worksource, as soon as the WSC is ready to receive employment confirmation events from the channel sink
          ; (outlet= this.hcEmployed )|() =>* peer.postCommand( expr

            OfferHeads( {$ senv.swarmManagerPid $}, {$ this.hcReserved $} )

          )

          # start the team keeper, it'll subscribe to this.hcEmployed, thus to kick off the producer above
          that.startTeamKeeper()


          {

            while peer.eol() is false case peer.readCommand() of {@
              # sequence of commands each resulting in nil are expected and let pass here
              { cmdVal } -> {
                console.warn<| 'Unexpected cmd from ' ++ peer ++ '\n  '
                ++ desc(cmdVal)
              }
            @}

          } $=> { exc } -> {
            console.error<| 'Exception by worksource ' ++ peer ++ ' error: '
            ++ desc(exc)
          }

        }
        clnt =: Client()
        # note @.addrs()@ will wait until it's connected or failed
        case clnt.addrs() of { addr :>_ }
        -> console.debug<| 'Connected to worksource at: ' ++ addr
        if clnt.eol() is not false then {
          clnt.join() # this usually rethrows the causing error, but let's still report an error if it didn't
          error( 'connection to worksource failed' )
        }

        # wait for explicit disconnection
        clnt.join()
      }

      # register this wsc to the forager
      this.forager.wsd[ this.key() ] = that

    } $=> { exc } -> { # this can catch exceptions occurred synchronously, or more probably in case here, asynchronously on the forked negotiating thread
      console.error<| 'Stopping team ' ++this++ ' on error: ' ++ desc( exc )

      this.stop() # cleanup on any exception occurred
    }

    return this.hcReserved
  }

  # this method meant to be run once per wsc
  method startTeamKeeper() go {
    # terminate this thread on stop signal
    perceive this.stopped { break }

    # track delta of employment
    hcEmployed = 0
    go for hcConfirmed from this.hcEmployed do {
      if hcConfirmed > this.hcReserved then error (
        'Worksource ' ++ this.addr ' confirming ' ++ hcConfirmed
        ++ ' while only ' ++ this.hcReserved ++ ' reserved.'
      )
      console.debug<| 'Worksource ' ++ this.addr ++ ' confirmed '
      ++ hcConfirmed ++ ' heads employed.'
      # TODO any deal with the delta ?
      hcEmployed = hcConfirmed
      # request reform
      this.reformSignal <- ()
    }

    # do reform upon appropriate chances
    while true {
      # wait next chance to reform
      for () from this.reformSignal do break
      # keep enough worker processes running
      while this.workers.size < hcEmployed {
        wkrPid = wscStartWorker(
          this.addr,
          this.workDir, this.executable, this.workModu,
        )
        this.workers[ wkrPid ] = this
        this.forager.workerStarted <- ( wkrPid, this )
      }
    }
  }

  method stop() {
    this.hcEmployed <-nil
    this.stopped <- true

    # forcefully stop all workers for this worksource
    for ( pid, _ ) from this.workers do {
      # make sure this worker process is killed
      killWorker( pid )
      this.workers[ pid ] = nil # forget about it
    }

    # check unleash heads reserved for this worksource
    ai if this.hcReserved > 0 then {
      this.forager.hcBackIdle <- this.hcReserved
      this.hcReserved = 0
    }
    # forget about this worksource
    ai case this.forager.wsd[ this.key() ] of this -> {
      this.forager.wsd[ this.key() ] = nil
    }
  }

  method update (
    hcPlanned, executable, workDir, workModu, priority,
  ) {
    # TODO check and deal with expected / unexpected changes
    this.hcPlanned = hcPlanned
    this.executable = executable
    this.workDir = workDir
    this.workModu = workModu
    this.priority = priority
  }

}


export class Forager {

  method __init__( headcount as this.headcount, ) {
    this.headcount >= 1 or error( 'Invalid headcount: ' ++this.headcount )
    this.hdIdle = this.headcount

    # prioritized recent queue for backlog of worksources
    this.wsq = PRQ()
    # worksources currently being actively worked on
    this.wsd = {}

    # available headcounts to be offered
    this.hcBackIdle = sink
    # stream of `pid:wsc` for ever started worker processes
    this.workerStarted = sink
  }

  # A work source (e.g. a head hunter) should announce call-for-workers
  # in this format, and this intends to be exposed (imported) by a network
  # facing entry module of the sniffer, so as to react to cfws
  export method WorkToDo (
    hcPlanned, # planned headcount to recruit
    executable, # executable file path to launch as worker processes
    workDir, # working directory
    workModu, # work definition module
    priority = 0, # used to prioritize allocation of work heads
  ) {
    # apply any filter in effect
    # todo only filtering by work modu so far, elaborate in the future
    if perform ignoreWorkModu( workModu ) then return nil

    # we are supposed to connect to the source address of the announced
    # call-for-workers (UDP packet), to comm with the announcing worksource
    #
    # and the sniffer (initiated for this forager) should be providing the
    # effectful `sourceAddr`
    cfwAddr = perform sourceAddr

    # such addr objects are non-equal from eachother in object semantics,
    # even from the same advertiser, so we use its repr str as identifier
    wsKey = repr( cfwAddr )
    console.debug<| ( 'Got call for '++hcPlanned++' worker(s), with currently '
      ++this.hdIdle++' head(s) idle.')
    ai {
      case this.wsd[ wsKey ] of {
        { _ } -> pass # already been worked on TODO update info ?
        case this.wsq.iqd[ wsKey ] of {
          { wsc } -> {
            # already in queue, refresh its known states up-to-date
            wsc.update( hcPlanned, executable, workDir, workModu, priority, )
          }
          this.wsq.enque( WSC (
              forager=this, addr=cfwAddr,
              hcPlanned, executable, workDir, workModu, priority,
          ) )
        }
      }
    }

    return nil
  }

  method scheduleTeams() {

    go {@
      # maintain mapping from pid of live worker processes to its team manager,
      # and track done of worker processes as well
      workers = {}

      perceive this.workerStarted {
        nil -> { break } # assuming it'll go eos upon forager shutdown
        # make sure it's remembered
        # note this perceiver preempts the loop below, so no race
        { ( wkrPid, wsc ) } -> workers[ wkrPid ] = wsc
      }

      # for each worker process started, wait one exited
      for (***) from this.workerStarted
      do case waitAnyWorkerDone() of {( donePid:doneAct:doneDesc )} -> {
        console.debug<| 'Swarm worker pid=' ++ donePid ++ ' ' ++ doneAct
        ++ ' - ' ++ doneDesc
        case workers[ donePid ] of { doneWSC } -> ai {
          # uncount this one from the team's number of working heads
          doneWSC.workers[ donePid ] = nil
          # request reform of the team
          doneWSC.reformSignal <- ()
          workers[ donePid ] = nil # forget about it
        }
      }
    @}

    # start out to have the configured total headcount available
    nsLastIdle = 0
    # collect idle heads in a dedicated thread, to not miss any unleash
    go for hcNewIdle from this.hcBackIdle do {
      if hcNewIdle < 1 then continue
      nsLastIdle = console.now()
      ai this.hdIdle += hcNewIdle
    }
    # insert some zero idle counts periodicly, so the check in following loop
    # won't dead wait in case it can only partial fill for a long time
    go for _ from console.everySeconds( 1 ) do this.hcBackIdle <- 0

    # engage with next worksource in the queue of discovery
    for wsc from this.wsq.streamOut() do {
      console.debug<| 'Discovered new worksource: ' ++ wsc.addr

      # do wait if can not immediately fulfill all the heads
      if this.hdIdle < this.headcount and this.hdIdle < wsc.hcPlanned
      then for _ from this.hcBackIdle
      # can fulfill now
      do ( this.hdIdle >= wsc.hcPlanned or this.hdIdle >= this.headcount )
      # or waited long enough while partial filling is possible
      # TODO make the hardcoded 5 seconds tunable
      || ( this.hdIdle > 0 and console.now() - nsLastIdle > 5e9 )
      -> { break }

      console.debug<| 'Engaging worksource ' ++ wsc ++ ' with '
      ++ this.hdIdle ++ ' heads to offer.'

      # engage with this worksource with as many free heads atm
      # `engage()` should return quickly without waiting for network,
      # network operations are actually forbidden by the intrinsic tx of
      # assignment
      this.hdIdle -= wsc.engage( this.hdIdle )
    }

    # todo this seems unreachable, add shutdown trigger?
    # shutdown forager, mark eos for various event sinks
    this.hcBackIdle <-nil
    this.workerStarted <-nil
  }

}
