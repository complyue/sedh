
import * 'net'

import * 'swarm/RT'

import * './wsq'

# worksource connection, and the team of workers employed by it
export class WSC {

  hcEmployed = 0
  workers = {}  # worker processes by pid

  method __init__ (
    forager as this.forager,
    addr as this.addr, 
    hcPlanned as this.hcPlanned,
    executable as this.executable,
    workDir as this.workDir,
    priority as this.priority,
  ) pass

  method __repr__() {
    '<WorkTeamFor ' ++ this.addr ++ ' #' ++ this.hcEmployed ++ '>'
  }

  method key() repr(this.addr)

  peer = None

  method engage (hcAvailable) {
    ai {
      this.hcEmployed = min(hcAvailable, this.hcPlanned)
      this.forager.wsd[this.key()] = this
    }

    { go {
      method __init_nego__ () {
        this.peer = that.peer
        that.wsc = this
      }
      case Client(
        this.forager.negoModu, this.addr, init=__init_nego__,
      ) of -> { clnt } -> {
        case clnt.addrs() of { addr =>_ } ->
          console.debug<| 'Connected to worksource at: ' ++ addr
        case clnt.eol() of {
          false -> { pass }
          clnt.join()  # this normally throws
          error('connection to worksource failed')
        }
      }
      producer offerHeads(outlet) {
        this.peer.postCommand(expr

OfferHeads({$ swarmManagerPid $}, {$ this.hcEmployed $})

        )
        # wait until disconnected from worksource, then mark eos to stop
        # the hc confirmation update loop, or stm deadlock would kill us
        this.peer.join() @=> outlet <- nil
      }
      for hcComfirmed from offerHeads(this.peer.armChannel(dataChan)) do {
        hcConfirmed < 0 -> error (
          'Suspecious worksource confirming negative hc: ' ++ hcConfirmed
        )
        ai {
          hcSpared = this.hcEmployed - hcConfirmed
          hcSpared < 0 -> error (
            'Suspecious worksource confirming excessive hc: ' ++ hcConfirmed
          ) 
          if hcSpared > 0 then {
            this.hcEmployed -= hcSpared
            this.forager.hcIdle <- hcSpared
          }
        }
        # TODO manage worker processes accordingly

      }
    } } $=> { exc } -> {  # this catches exceptions occurred directly or
      # in forked threads
      console.error<| 'Stopping team '++this++' on error: ' ++ exc
      this.stop()
    }

    return this.hcEmployed
  }

  method stop() {
    # TODO forcefully stop all workers for this worksource

    # check releash occupied heads
    wsKey = this.key()
    ai case this.forager.wsd[wsKey] of {
      nil -> { pass }  # already unleashed, don't repeat doing
      # releash headcount occupied by this team
      this.forager.wsd[wsKey] = nil
      this.forager.hcIdel <- this.hcEmployed
    }
  }

  method update (
    hcPlanned, executable, workDir, priority,
  ) {
    # TODO check and deal with expected / unexpected changes
    this.hcPlanned = hcPlanned
    this.executable = executable
    this.workDir = workDir
    this.priority = priority
  }

}


export class Forager {

  # available headcounts to be offered
  hcIdle = sink
  # prioritized recent queue for backlog of worksources
  wsq = PRQ()
  # worksources currently being actively worked on
  wsd = {}

  method __init__(
    headcount as this.headcount,
    negoModu as this.negoModu = 'swarm/negotiator',
  ) {
    this.headcount >= 1 |> error('Invalid headcount: '++this.headcount)
  }

  # A work source (e.g. a head hunter) should announce call-for-workers
  # in this format, and this intends to be exposed (imported) by a network
  # facing entry module of the sniffer, so as to react to cfws
  export method WorkToDo (
    hcPlanned,     # planned headcount to recruit
    executable,    # executable file path to launch as worker processes
    workDir,       # working directory
    priority = 0,  # used to prioritize allocation of work heads
    target = '',   # for prefix based filtering of uninteresting work
  ) {
    case target of {
      { that.targetPrefix >@ _ } -> { pass }
      return nil  # filtered out by prefix
    }

    # `that.addr` is the worksource address we are supposed to connect,
    # which is from the network facing module of a sniffer i.e. the forager
    #
    # such addr objects are non-equal from eachother in object semantics,
    # even from the same advertiser, so we use its repr str as identifier
    wsKey = repr(that.addr)
    ai {
      case this.wsd[wsKey] of {
        { _ } -> { pass }  # already been worked on
        case this.wsq.iqd[wsKey] of {
          { wsc } -> {
            # already in queue, refresh its known states up-to-date
            wsc.update( hcPlanned, executable, workDir, priority, )
          }
          this.wsq.enque(WSC(
            forager=this, addr=that.addr,
            hcPlanned, executable, workDir, priority,
          ))
        }
      }
    }
  }

  method scheduleTeams() {
    hcCum = 0 nsLastIdle = 0
    # let it start out to be the configured total headcount,  there won't
    # be further post to it until some heads are offered then unleashed
    this.hcIdle <- this.headcount
    # collect idle heads in a dedicated thread, to not miss any unleash
    go for hcNewIdle from this.hcIdle do {
      hcNewIdle < 1 -> { continue }
      nsLastIdle = console.now()
      hcCum += hcNewIdle
    }
    # insert some zero counts, so the check in following loop won't dead
    # wait in case it can only partial fill for a long time
    go for _ from console.everySeconds(1) do this.hcIdle <- 0

    for wsc from this.wsq.streamOut() do {
      # wait until got fulfilling free heads, or waited long enough while
      # partial filling is possible
      # TODO make the hardcoded 5 seconds tunable
      for _ from this.hcIdle do
        (hcCum >= wsc.hcPlanned || hcCum >= this.headcount)
        || (hcCum > 0 && console.now() - nsLastIdle > 5e9)
        -> { break }

      # engage with this worksource with as many free heads atm
      # `engage()` should return quickly without waiting for network,
      # network operations are actually forbidden by the intrinsic tx of
      # assignment
      hcCum -= wsc.engage(hcCum)
    }
  }

  method calibrateEmployment() {
    hcEmployed = 0
    ai for (wsKey, wsc) from this.wsd do case wsc.eol() of {
      false -> hcEmployed += wsc.hcEmployed
      # or forget this wsc as already end-of-life, i.e. disconnected
      this.wsd[wsKey] = nil
    }
    return hcEmployed
  }

  method xxx() {
    hcEmployed = calibrateEmployment()
    (hcAvailable = this.headcount - this.hcEmployed)
    < 1 -> { pass }  # no free head available, ignore this cfw

    # `that.addr` is the worksource address we are supposed to connect,
    # which is from the network facing module of a sniffer i.e. the forager
    #
    # such addr objects are non-equal from eachother in object semantics,
    # even from the same advertiser, so we use its repr str as identifier
    wsKey = repr(that.addr)

    # see it's a known worksource, a new wip connection, or a fresh new
    # worksource, handle accordingly
    ai case this.wsd[wsKey] of {
      {{ Addr: _ }} ->  # another connection attempt to it is being made
        return nil
      {{ WSC: wsc }} -> {
        # let the previous wsc object handle the update, even if it has
        # disconnected, it needs to track running worker processes for
        # proper retirement, then clear the wsc record from `wsd`.
        # we won't attempt new connections to a worksource until last
        # connection is all clean
        go wsc.update( hcPlanned, executable, workDir, priority, )
        return nil  # nothing more to do here
      }
      # so we are going to make a new connection to it
      # TODO use a priority queue to prioritize among worksources
      ( hcReserved = min( hcPlanned,
        max(0, this.headcount - this.hcEmployed) )
      ) > 0 -> {
        this.hcEmployed -= hcReserved
        wsd[wsKey] = that.addr
      }
    }
    not hcReserved > 0 -> { pass }

    {
      case WSC(
        forager=this, addr=that.addr, hcReserved=hcReserved,
        hcPlanned, executable, workDir, priority,
      ) of { wsc } -> wsd[wsKey] = wsc
    } $=> { wsExc } -> {
      # NOTE this handles exceptions occured in spawned goroutines as well,
      #      run in such a goroutine's thread in that case
      console.error<| 'Forager failed contacting worksource at '
        ++ that.addr ++ '\n' ++ wsExc
    } @=> {
      # check if we actually made it, unleash the reservation if not
      ai case wsd[wsKey] of that.addr -> {
        # forget about it
        wsd[wsKey] = nil
        # unleash the reservation
        this.hcEmployed += hcReserved
      }
    }

  }

}

