
import * 'net'

import * 'swarm/RT'

# work definition scripts are allowed to change the inferred
# configuration at 'swarm/ENV', import it as a namespace to
# always use up-to-date artifacts living there
case { import (**_) 'swarm/ENV' } of { senv } -> { pass }

import * './wsq'



# worksource connection, and the team of workers employed by it
export class WSC {

  method __init__ (
    forager as this.forager,
    addr as this.addr, 
    hcPlanned as this.hcPlanned,
    executable as this.executable,
    workDir as this.workDir,
    priority as this.priority,
  ) pass

  # headcount reserved for this worksource
  hcReserved = 0
  # headcount hired by the worksource, updated from the worksource,
  # as its demands changed
  hcEmployed = sink

  # all live worker processes by pid
  workers = {}

  method __repr__() {
    '<WorkTeamFor ' ++ this.addr ++ ' ' ++ this.workers.size ++ '/'
      ++ mre(this.hcEmployed) ++ '/' ++ this.hcReserved ++ '>'
  }

  method key() repr(this.addr)

  # connection to the worksource (headhunter) identified as a forager
  peer := Peer()

  method engage (hcAvailable) {
    hcReserved = min(hcAvailable, this.hcPlanned)

    {
      go {
        method __init_nego__ () {
          this.peer = that.peer
          that.wsc = this
        }
        case Client(
          this.forager.negoModu, this.addr, init=__init_nego__,
        ) of -> { clnt } -> {
          case clnt.addrs() of { addr =>_ } ->
            console.debug<| 'Connected to worksource at: ' ++ addr
          case clnt.eol() of {
            false -> { pass }
            clnt.join()  # this normally throws
            error('connection to worksource failed')
          }
        }
        producer offerHeads(outlet) {
          this.peer.postCommand(expr

  OfferHeads({$ swarmManagerPid $}, {$ hcReserved $})

          )
        }
        offerHeads(this.peer.armChannel(dataChan, this.hcEmployed))

        # start the team keeper, it'll subscribe to this.hcEmployed, thus
        # trigger the offerHeads() producer
        go this.teamKeeper()

        # stop this wsc anyway the network connection is disconnected
        clnt.join() @=> this.stop()
      }

      # register this wsc
      this.forager.wsd[this.key()] = this
      # materialize the reservation
      this.hcReserved = hcReserved
    } $=> { exc } -> {  # this catches exceptions occurred directly or
      # in forked threads
      console.error<| 'Stopping team '++this++' on error: ' ++ exc
      this.stop()
    }

    return this.hcReserved
  }

  # stop signal for this wsc
  stopped := sink

  # ever increasing number to track reform bumps
  reformNumber := sink
  reformNumber <- 0  # initialize to 0

  # this method meant to be forked once per wsc
  method teamKeeper() {
    # terminate this thread on stop signal
    perceive stopped { break }

    rfn = mre(this.reformNumber)

    # track delta of employment
    hcEmployed = 0
    go for hcConfirmed from this.hcEmployed do {
      if hcConfirmed > this.hcReserved then error (
        'Worksource ' ++ this.addr ' confirming ' ++ hcConfirmed 
        ++ ' while only ' ++ this.hcReserved ++ ' reserved.'
      )
      # TODO any deal with the delta ?
      hcEmployed = hcConfirmed
      ai {  # request reform
        this.reformNumber <- mre(this.reform) + 1
      }
    }

    # do reform upon appropriate chances
    while true {
      # wait next chance to reform
      for rfn' from this.reformNumber do rfn' != rfn -> {
        rfn = rfn'
        break
      }
      # keep enough worker processes running
      while this.workers.size < hcEmployed {
        case wscStartWorker(
          this.addr,
          senv.jobWorkDir, senv.jobExecutable, senv.jobWorkModu,
        ) of { wkrpid } -> {
          this.workers[wkrpid] = this
          this.forager.workers <- pkargs(wkrpid, this)
        }
      }
    }
  }

  method stop() {
    this.hcEmployed <- nil
    this.stopped <- nil

    # forcefully stop all workers for this worksource
    for (pid, wkrp) from this.workers do while true {
      # TODO make sure this worker process is killed

      this.workers[pid] = nil  # forget about it
      break  # proceed only after kill confirmed, it's better to wait forever
      # than exhausting the system by creating too many new worker processes
    } $=> { exc } -> {
      console.error<| 'Failed killing worker process pid=' ++ pid
      # wait a second on error killing a worker process
      for _ from console.everySeconds(1) do { break }
    }

    # check unleash heads reserved for this worksource
    ai if this.hcReserved > 0 then {
      this.forager.hcIdle <- this.hcReserved
      this.hcReserved = 0
    }
    # forget about this worksource
    ai case this.forager.wsd[this.key()] of this -> {
      this.forager.wsd[this.key()] = nil
    }
  }

  method update (
    hcPlanned, executable, workDir, priority,
  ) {
    # TODO check and deal with expected / unexpected changes
    this.hcPlanned = hcPlanned
    this.executable = executable
    this.workDir = workDir
    this.priority = priority
  }

}


export class Forager {

  method __init__(
    headcount as this.headcount,
    negoModu as this.negoModu = 'swarm/negotiator',
  ) {
    this.headcount >= 1 |> error('Invalid headcount: '++this.headcount)
  }

  # prioritized recent queue for backlog of worksources
  wsq = PRQ()
  # worksources currently being actively worked on
  wsd = {}

  # A work source (e.g. a head hunter) should announce call-for-workers
  # in this format, and this intends to be exposed (imported) by a network
  # facing entry module of the sniffer, so as to react to cfws
  export method WorkToDo (
    hcPlanned,     # planned headcount to recruit
    executable,    # executable file path to launch as worker processes
    workDir,       # working directory
    priority = 0,  # used to prioritize allocation of work heads
    target = '',   # for prefix based filtering of uninteresting work
  ) {
    case target of {
      { that.targetPrefix >@ _ } -> { pass }
      return nil  # filtered out by prefix
    }

    # `that.addr` is the worksource address we are supposed to connect,
    # which is from the network facing module of a sniffer i.e. the forager
    #
    # such addr objects are non-equal from eachother in object semantics,
    # even from the same advertiser, so we use its repr str as identifier
    wsKey = repr(that.addr)
    ai {
      case this.wsd[wsKey] of {
        { _ } -> { pass }  # already been worked on TODO update info ?
        case this.wsq.iqd[wsKey] of {
          { wsc } -> {
            # already in queue, refresh its known states up-to-date
            wsc.update( hcPlanned, executable, workDir, priority, )
          }
          this.wsq.enque(WSC(
            forager=this, addr=that.addr,
            hcPlanned, executable, workDir, priority,
          ))
        }
      }
    }
  }

  # available headcounts to be offered
  hcIdle = sink
  # stream of `pid:wsc` for ever started worker processes 
  workerStarted = sink

  method scheduleTeams() {

    # maintain mapping from pid of live worker processes to its team manager
    workers = {}
    go for (wkrpid, wsc) from this.workerStarted do workers[wkrpid] = wsc

    # track done of worker processes
    go for _ from this.workerStarted do case waitAnyWorkerDone()
    of { donePid:doneAct:doneDesc } -> {
      console.debug<| 'Swarm worker pid=' ++ donePid ++ ' ' ++ doneAct
        ++ ' - ' ++ doneDesc
      case workers[donePid] of { doneWSC } -> ai {
        # uncount the team's number of working heads
        doneWSC.workers[donePid] = nil
        # request reform of the team
        doneWSC.reformNumber <- mre(doneWSC.reform) + 1
        workers[donePid] = nil  # forget about it
      }
    }

    hcCum = 0 nsLastIdle = 0
    # let it start out to be the configured total headcount,  there won't
    # be further post to it until some heads are offered then unleashed
    this.hcIdle <- this.headcount
    # collect idle heads in a dedicated thread, to not miss any unleash
    go for hcNewIdle from this.hcIdle do {
      hcNewIdle < 1 -> { continue }
      nsLastIdle = console.now()
      hcCum += hcNewIdle
    }
    # insert some zero counts periodicly, so the check in following loop won't
    # dead wait in case it can only partial fill for a long time
    go for _ from console.everySeconds(1) do this.hcIdle <- 0

    # engage with next worksource in the queue of discovery
    for wsc from this.wsq.streamOut() do {
      # wait until got fulfilling free heads, or waited long enough while
      # partial filling is possible
      # TODO make the hardcoded 5 seconds tunable
      for _ from this.hcIdle do
        (hcCum >= wsc.hcPlanned || hcCum >= this.headcount)
        || (hcCum > 0 && console.now() - nsLastIdle > 5e9)
        -> { break }

      # engage with this worksource with as many free heads atm
      # `engage()` should return quickly without waiting for network,
      # network operations are actually forbidden by the intrinsic tx of
      # assignment
      hcCum -= wsc.engage(hcCum)
    }

  }

}

