
import * 'math'

import * 'net'

import * 'swarm/RT'

# work definition scripts are allowed to change the inferred configuration at
# 'swarm/ENV', import the module as a reference, to get/set effective artifacts
# living there
senv = { import _ 'swarm/ENV' }

import * './wsq'


{## worksource connection, and the team of workers employed by it #}
export class WSC {

  method __init__ (
    forager as this.forager,
    addr as this.addr,
    hcPlanned as this.hcPlanned,
    executable as this.executable,
    workDir as this.workDir,
    workModu as this.workModu,
    priority as this.priority,
  ) {
    this.forager =: Forager()

    # headcount reserved for this worksource
    this.hcReserved = 0
    # headcount hired by the worksource
    this.hcEmployed = 0
    # channel for updates from the worksource, as its demands changed
    this.hcEmployedUpd = chan

    # all live worker processes by pid
    this.workers = {}

    # signals for reform requests
    this.reformSignal = Semaphore()

    # stop signal (by eos) for this wsc
    this.stopped = chan
  }

  method __repr__() {
    '<WorkTeamFor ' ++ this.addr ++ ' ' ++ this.workers.size ++ '/'
    ++ this.hcEmployed ++ '/' ++ this.hcReserved
    ++ '/' ++ this.hcPlanned ++ '>'
  }

  method key() repr( this.addr )

  method engage ( hcAvailable ) {
    this.hcReserved = min( hcAvailable, this.hcPlanned )

    { exc } -> { # this can catch exceptions occurred synchronously, or more probably in case here, asynchronously on the forked negotiating thread

      console.error<| 'Stopping team ' ++this++ ' on error: ' ++ desc( exc )

      this.stop() # cleanup on any exception occurred

    } <=$ {

      go { # do the negotiation on a separate thread, don't block the forager from comm with other worksources concurrently

        # necessary cleanup after it's disconnected anyway
        defer this.stop()

        # make the connection
        let clnt = Client(
          addrSpec= this.addr,
          useSandbox= false, # trade security mitigation for simplicity
        ) $@ () => {
          import * that # expose all exported instance methods

          peer = perform @netPeer
          peer =: Peer()
          console.debug<| 'Swarm forager negotiating with worksource - ' ++ peer
          defer {
            console.debug<| 'Disconnecting worksource ' ++ peer
          }

          # identify ourself as a forager to the worksource
          peer.armChannel( dataChan, this.hcEmployedUpd )
          peer.postCommand( expr
            OfferHeads( {$ senv.swarmManagerPid $}, {$ this.hcReserved $} )
          )

          # start the team keeper
          that.startTeamKeeper()

          {

            while peer.eol() is false case peer.readCommand() of {@
              # sequence of commands each resulting in nil are expected and let pass here
              { cmdVal } -> {
                console.warn<| 'Unexpected cmd from ' ++ peer ++ '\n  '
                ++ desc(cmdVal)
              }
            @}

          } $=> { exc } -> {
            console.error<| 'Exception by worksource ' ++ peer ++ ' error: '
            ++ desc(exc)
          }

        }
        clnt =: Client()
        # note @.addrs()@ will wait until it's connected or failed
        case clnt.addrs() of { addr :>_ }
        -> console.debug<| 'Connected to worksource at: ' ++ addr
        if clnt.eol() is not false then {
          clnt.join() # this usually rethrows the causing error, but let's still report an error if it didn't
          error( 'connection to worksource failed' )
        }

        # wait for explicit disconnection
        clnt.join()
      }

      # register this wsc to the forager
      this.forager.wsd[ this.key() ] = that

    }

    return this.hcReserved
  }

  # this method meant to be run once per wsc
  method startTeamKeeper() go {
    perceive nil this.stopped break # terminate this thread on stop signal

    # track delta of employment
    hcEmployed = 0
    go for hcConfirmed from this.hcEmployedUpd do {
      if hcConfirmed > this.hcReserved then error (
        'Worksource ' ++ this.addr ' confirming ' ++ hcConfirmed
        ++ ' while only ' ++ this.hcReserved ++ ' reserved.'
      )
      console.debug<| 'Worksource ' ++ this.addr ++ ' confirmed '
      ++ hcConfirmed ++ ' heads employed.'
      # TODO any deal with the delta ?
      hcEmployed = this.hcEmployed = hcConfirmed
      # request reform
      this.reformSignal.inc()
    }

    # do reform upon appropriate chances
    while true {
      # wait next chance to reform
      this.reformSignal.waitAny()
      # keep enough worker processes running
      while this.workers.size < hcEmployed {
        wkrPid = wscStartWorker(
          this.addr,
          this.workDir, this.executable, this.workModu,
        )
        this.workers[ wkrPid ] = that
        this.forager.workerStarted <- ( wkrPid, that )
      }
    }
  }

  method stop() {
    this.hcEmployedUpd <-nil
    this.stopped <-nil

    # forcefully stop all workers for this worksource
    for ( pid, _ ) from this.workers do {
      # make sure this worker process is killed
      killWorker( pid )
      this.workers[ pid ] = nil # forget about it
    }

    # check unleash heads reserved for this worksource
    if this.hcReserved > 0 then {
      this.forager.hcIdle.inc(this.hcReserved)
      this.hcReserved = 0
    }
    # forget about this worksource
    ai case this.forager.wsd[ this.key() ] of this -> {
      this.forager.wsd[ this.key() ] = nil
    }
  }

  method update (
    hcPlanned, executable, workDir, workModu, priority,
  ) {
    # TODO check and deal with expected / unexpected changes
    this.hcPlanned = hcPlanned
    this.executable = executable
    this.workDir = workDir
    this.workModu = workModu
    this.priority = priority
  }

}


export class Forager {

  method __init__( headcount as this.headcount, ) {
    this.headcount >= 1 or error( 'Invalid headcount: ' ++this.headcount )

    # headcount(s) available to be offered
    this.hcIdle = Semaphore(this.headcount)

    # prioritized recent queue for backlog of worksources
    this.wsq = PRQ()
    # worksources currently being actively worked on
    this.wsd = {}

    this.lastPriority = -1

    # stream of `pid:wsc` for ever started worker processes
    this.workerStarted = chan
    # waitable count of newly started workers
    this.workerStartedCnt = Semaphore()

    # stop signal (by eos) for this forager
    this.stopped = chan
  }

  # A work source (e.g. a head hunter) should announce call-for-workers
  # in this format, and this intends to be exposed (imported) by a network
  # facing entry module of the sniffer, so as to react to cfws
  export method WorkToDo (
    hcPlanned, # planned headcount to recruit
    executable, # executable file path to launch as worker processes
    workDir, # working directory
    workModu, # work definition module
    priority = 0, # used to prioritize allocation of work heads
  ) void {
    # apply any filter in effect
    # todo only filtering by work modu so far, elaborate in the future
    if perform ignoreWorkModu( workModu ) then return nil

    # we are supposed to connect to the source address of the announced
    # call-for-workers (UDP packet), to comm with the announcing worksource
    #
    # and the sniffer (initiated for this forager) should be providing the
    # effectful `sourceAddr`
    cfwAddr = perform sourceAddr

    # such addr objects are non-equal from eachother in object semantics,
    # even from the same advertiser, so we use its repr str as identifier
    wsKey = repr( cfwAddr )
    console.debug<| 'Got call for ' ++hcPlanned++ ' worker(s), with currently '
    ++this.hcIdle.inv++ ' head(s) idle.'
    ai {
      case this.wsd[ wsKey ] of {
        { _ } -> pass # already been worked on TODO update info ?
        case this.wsq.iqd[ wsKey ] of {
          { wsc } -> {
            # already in queue, refresh its known states up-to-date
            wsc.update( hcPlanned, executable, workDir, workModu, priority, )
          }
          this.wsq.enque( WSC (
              forager=this, addr=cfwAddr,
              hcPlanned, executable, workDir, workModu, priority,
          ) )
        }
      }
    }
  }

  {## this is the long-running method of a forager service #}
  method scheduleTeams() {

    defer { # upon forager shutdown, mark eos for various channels
      this.workerStarted <-nil
      this.stopped <-nil
    }

    go {@
      # maintain mapping from pid of live worker processes to its team manager,
      # and track done of worker processes as well
      workers = {}

      go for@ ( wkrPid, wsc ) from this.workerStarted do {
        workers[ wkrPid ] = wsc # make sure it's remembered
        this.workerStartedCnt.inc() # count for one
      }

      perceive nil this.stopped break # eos upon forager shutdown

      while true { # for each worker process started, wait one exited
        # possible blocking intended, consume one from the counting semaphore
        this.workerStartedCnt.wait()
        # wait any os worker process exit
        case waitAnyWorkerDone() of {( donePid:doneAct:doneDesc )} -> {
          console.debug<| 'Swarm worker pid=' ++ donePid ++ ' ' ++ doneAct
          ++ ' - ' ++ doneDesc
          case workers[ donePid ] of { doneWSC } -> ai {
            doneWSC =: WSC()
            # uncount this one from the team's number of working heads
            doneWSC.workers[ donePid ] = nil
            # request reform of the team
            doneWSC.reformSignal.inc()
            workers[ donePid ] = nil # forget about it
          }
        }
      }
    @}

    # engage with next worksource in the queue of discovery
    for wsc: WSC from this.wsq.streamOut() do {
      console.debug<| 'Discovered new worksource: ' ++ wsc.addr

      ;| wsc.priority < this.lastPriority -> {
        console.debug<| 'Giving a chance to the priority ' ++ this.lastPriority
        ++ ' task just done to come back, before engaging this priority '
        ++ wsc.priority ++ ' task.'
        # wait a minute if the next up queued task is of lower priority than
        # we've just done, give chance for higher priority tasks in a batch
        # fashion (i.e. multiple iterations with slight gap time) to hold the
        # heads before lower priority tasks get to run
        for _ from console.everySeconds(60) do break
        this.lastPriority = wsc.priority
      }

      hcDemand = min( wsc.hcPlanned, this.headcount )

      # first try fulfill the demand as a whole
      case this.hcIdle.wait(
        hcDemand, # try consume exactly that much demanded
        timeout= 5e6, # TODO make the hardcoded 5 seconds tunable
      ) of {_} -> { # full fill granted
        console.debug<| 'Fully engaging worksource ' ++ wsc ++ ' with ' ++ hcDemand ++ ' heads to offer.'
        this.lastPriority = wsc.priority
        hcEngaged = wsc.engage( hcDemand )
        if ( hcSpared = hcDemand - hcEngaged ) > 0 then {
          this.hcIdle.inc(hcSpared)
        }
        continue
      }

      console.debug<| 'Full fill not possible, trying a partial fill ...'
      pwsTime = console.now()
      hcBackIdle = this.hcIdle.waitAny()
      assert$ hcBackIdle > 0
      ;| console.now() - pwsTime > 10e9 -> { # TODO make the 10 seconds threshold tunable
        console.debug<| "It has been a while since started waiting idle heads for this worksource, I'm dropping it and to see what's off the top of my queue now."
      }
      console.debug<| 'Partially engaging worksource ' ++wsc++ ' with ' ++hcBackIdle++ ' heads to offer.'
      this.lastPriority = wsc.priority
      hcEngaged = wsc.engage( hcBackIdle )
      if ( hcSpared = hcBackIdle - hcEngaged ) > 0 then {
        this.hcIdle.inc(hcSpared)
      }

    }

  }

}
