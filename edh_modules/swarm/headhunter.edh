
import * 'net'

import * 'swarm/RT'

import * './debug'
import * './symbols'

# work definition scripts are allowed to change the inferred configuration at
# 'swarm/ENV', import the module as a reference, to get/set effective artifacts
# living there
senv = import _ 'swarm/ENV'


export class HeadHunter {

  data Forager ( peer, pid ) {
    hcEmployed = 0
    hcWorking = 0
  }

  class Worker {
    method __init__(
      peer as this.peer,
      workerPid as this.pid,
      foragerPid as this.manager,
    ) {
      errSink = this.peer.armChannel( errChan )
      # convert disconnection without result to error
      go {
        discExc = None
        this.peer.join() $=> {
          { discExc } -> pass
        }
        errSink <- discExc or PeerError( 'Swarm worker disconnected' )
      }
    }

    method __repr__() {
      '<Worker pid=' ++ this.pid ++ ' via ' ++ this.peer.ident ++ '>'
    }
  }

  # a swarm node connection identifying itself as a forager
  export method OfferHeads(
    foragerPid, maxHC,
  ) {
    peer = perform @netPeer

    ai { # calculate number of employment atomically
      hcDemand = this.headcount - this.hcEmployed
      hcToEmploy = min( maxHC, hcDemand )
    }
    hcToEmploy < 1 -> { # no demand, disconnect
      # todo send zero employment ?
      peer.stop()
    }

    # employ heads from this forager
    ai case this.foragers[ peer ] of {
      nil -> {
        forager = Forager(
          peer, foragerPid,
        )
        this.foragers[ peer ] = forager
      }
      { forager } -> pass
    }
    forager.hcEmployed += hcToEmploy
    console.debug<| 'HH is hiring ' ++ forager.hcEmployed ++ ' heads from '
    ++ peer.ident ++ ' now.'
    peer.p2c( dataChan, repr( forager.hcEmployed ) )

    return nil
  }

  # a swarm node connection identifying itself as a worker
  export method StartWorking(
    workerPid, foragerPid,
  ) {
    peer = perform @netPeer

    console.debug<| 'Worker process pid=' ++ workerPid ++ ' managed by '
    ++ foragerPid ++ ' via ' ++ peer.ident ++ ' start working.'
    worker = Worker( peer, workerPid, foragerPid )
    this.idleWorkers.push( worker )
    this.workerAvailable <- worker
    return nil
  }

  method __init__( resultSink as this.resultSink = sink ) {
    # fetch effective configurations, cache as instance attribute
    this.priority = perform priority
    this.headcount = perform headcount
    this.workModu = perform @workDefinition

    # total heads employed by this HH
    this.hcEmployed = 0
    # data structures for job scheduling
    this.foragers = {}
    this.workerAvailable = sink # signal on new worker available,
    # but should obtain an available worker from idleWorkers with tx
    this.idleWorkers = [] # FILO queue
    this.pendingJobs = [] # FILO queue
    this.pendingCntr = sink # increased/decreased on job submit/result/err
    this.pendingCntr <- 0 # initialize to be zero
    this.finishingUp = sink # signal of no more jobs, i.e. to start finishing up

    go {
      hh = this; # alias self
      server = Server(
        'swarm/headhunter', # the service module
        perform swarmIface $=> '0.0.0.0', # local addr to bind
        0, # local port to bind
        init= modu => { # per-connection peer module initialization
          import * hh into modu
        },
      )
      case server.addrs() of {
        { wsAddr :>_ } -> console.info<| 'HeadHunter listening: ' ++ wsAddr
        # or the network has failed, propagate the error
        server.join() # this usually throws
        # in case join() didn't throw, report this error
        error( 'HeadHunter failed listening.' )
      }

      # call-for-workers advertiser
      cfw = Advertiser(
        # target swarm address
        perform swarmAddr $=> '127.0.0.1', perform swarmPort $=> 3722,
        # announce call-for-workers from our worksource addr
        fromAddr=wsAddr,
      )

      # advertise call-for-workers when there's demand
      for _ from console.everySeconds(
        perform cfwInterval $=> 3, # use effectful config
        wait1st=false, # announce the first CFW immediately
      ) do {
        # check if all have finished
        this.allFinished() -> { break }

        # calibrate this.hcEmployed, wrt forager disconnection etc.
        hcEmployed = 0
        for ( peer, forager ) from this.foragers
        do case peer.eol() of {
          false -> hcEmployed += forager.hcEmployed
          this.foragers[ peer ] = nil # forget about it
        }
        if this.hcEmployed != hcEmployed then {
          console.debug<| 'HH has ' ++ hcEmployed ++ ' heads employed now.'
          this.hcEmployed = hcEmployed
        }

        hcDemand = this.headcount - hcEmployed
        if hcDemand > 0 then {
          console.debug<| 'Calling ' ++ hcDemand ++ ' workers from '
          ++ cfw.addrs()
          cfw.post( expr

            WorkToDo(
              {$ hcDemand $},
              {$ senv.jobExecutable $},
              {$ senv.jobWorkDir $},
              {$ this.workModu $},
              {$ this.priority $},
            )

          )
        }
      }
    }
  }

  method allFinished() {
    this.resultSink.eos -> true

    if this.finishingUp.mrv is true
    && this.pendingCntr.mrv is 0
    && null( this.pendingJobs )
    then {
      this.resultSink <-nil
      return true
    }

    return false
  }

  method trackJob( worker, ips ) {
    # arm a new private data channel to this job
    jobSink = worker.peer.armChannel( dataChan )
    # perform retry effect on error
    perceive worker.peer.armedChannel( errChan ) { jobExc } -> {
      worker.peer.stop() # disconnect this worker anyway on error

      # call the effectful failure callback
      case perform @shouldRetryJob of {
        false -> pass
        true -> this.pendingJobs.push( ips )
        { retryChk } -> case retryChk( jobExc, ips ) of { newIPS } -> {
          this.pendingJobs.push( newIPS )
        }
      }

      # failed to get a result, but we know it's not pending now
      if jobSink.mrv is nil then {
        ai this.pendingCntr <- this.pendingCntr.mrv - 1
        jobSink <-nil
      }

      break # terminate this thread
    }
    # submit ips and process result
    for result from (outlet= jobSink )|() =>* {
      worker.peer.p2c( dataChan, repr( ips ) )
    } do {
      this.resultSink <- ( ips, result )
      ai this.pendingCntr <- this.pendingCntr.mrv - 1
      this.idleWorkers.push( worker )
      this.workerAvailable <- worker
      break # one ips at a time
    }
  }

  method submitJobs() {
    while true {
      ips = this.pendingJobs.pop()
      ips is None -> { return nil } # no more pending jobs
      ai this.pendingCntr <- this.pendingCntr.mrv + 1

      while true {
        # try grab a present idle worker and assign it this job
        worker = this.idleWorkers.pop()
        worker != None -> {
          console.debug<| 'Job assigned to ' ++ worker ++ ' - ' ++ ips
          # start asynchronous concurrency
          go trackJob( worker, ips )
          break # this job settled, proceed to next job
        }

        # no immediate idle worker, need to wait
        console.debug<| 'Wait for idle workers.'
        # wait until new idle workers appear
        for _ from this.workerAvailable do {
          not null( this.idleWorkers ) -> { break }
        }
        console.debug<| 'Got idle workers.'
      }
    }
  }

  method dispatchJob( ips ) {
    this.pendingJobs.push( ips )
    this.submitJobs()
  }

  method finishUp() {
    this.finishingUp <- true

    perceive this.pendingCntr.subseq 0 -> this.allFinished()

    for (*** _ ) from this.resultSink do { pass }
  }

}
